import json
import pandas as pd
import pyarrow.parquet as pq
import os

# Define paths
DATA_DIR = "HybridMalwareDetector/data/ember2018"
OUTPUT_DIR = "HybridMalwareDetector/data/processed"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Function to process and convert JSONL to Parquet in batches
def process_jsonl_in_batches(jsonl_file, output_file, batch_size=50000):
    print(f"Processing {jsonl_file} in batches of {batch_size}...")
    
    data_list = []
    with open(jsonl_file, "r") as file:
        for i, line in enumerate(file):
            data_list.append(json.loads(line))

            # Process in batches
            if (i + 1) % batch_size == 0:
                df = pd.DataFrame(data_list)
                df.to_parquet(f"{output_file}_part{i//batch_size}.parquet", engine="pyarrow")
                print(f"Saved batch {i//batch_size}")
                data_list = []  # Clear memory

        # Save remaining data
        if data_list:
            df = pd.DataFrame(data_list)
            df.to_parquet(f"{output_file}_final.parquet", engine="pyarrow")
            print("Final batch saved.")

# Process all JSONL files
jsonl_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".jsonl")]

for jsonl_file in jsonl_files:
    input_path = os.path.join(DATA_DIR, jsonl_file)
    output_path = os.path.join(OUTPUT_DIR, jsonl_file.replace(".jsonl", ""))
    process_jsonl_in_batches(input_path, output_path)

print("All files converted to Parquet successfully!")
